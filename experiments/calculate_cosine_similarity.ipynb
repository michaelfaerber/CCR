{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T21:01:07.328688Z",
     "iopub.status.busy": "2022-03-31T21:01:07.328159Z",
     "iopub.status.idle": "2022-03-31T21:01:09.151192Z",
     "shell.execute_reply": "2022-03-31T21:01:09.150486Z",
     "shell.execute_reply.started": "2022-03-31T21:01:07.328653Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
    "from transformers import RobertaConfig, RobertaModel,RobertaTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T10:19:51.119584Z",
     "iopub.status.busy": "2022-03-31T10:19:51.119038Z",
     "iopub.status.idle": "2022-03-31T10:19:51.125621Z",
     "shell.execute_reply": "2022-03-31T10:19:51.124749Z",
     "shell.execute_reply.started": "2022-03-31T10:19:51.119544Z"
    }
   },
   "outputs": [],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T10:19:53.314301Z",
     "iopub.status.busy": "2022-03-31T10:19:53.314034Z",
     "iopub.status.idle": "2022-03-31T10:19:53.375131Z",
     "shell.execute_reply": "2022-03-31T10:19:53.374251Z",
     "shell.execute_reply.started": "2022-03-31T10:19:53.31427Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T10:19:55.323096Z",
     "iopub.status.busy": "2022-03-31T10:19:55.322356Z",
     "iopub.status.idle": "2022-03-31T10:19:55.327782Z",
     "shell.execute_reply": "2022-03-31T10:19:55.327042Z",
     "shell.execute_reply.started": "2022-03-31T10:19:55.32305Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load agreement for 20 citing context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T10:19:57.633999Z",
     "iopub.status.busy": "2022-03-31T10:19:57.633445Z",
     "iopub.status.idle": "2022-03-31T10:19:57.645294Z",
     "shell.execute_reply": "2022-03-31T10:19:57.644443Z",
     "shell.execute_reply.started": "2022-03-31T10:19:57.633958Z"
    }
   },
   "outputs": [],
   "source": [
    "with open ('./annotations_agreement_20.json','r') as file:\n",
    "    annotation = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cited papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T21:01:11.335248Z",
     "iopub.status.busy": "2022-03-31T21:01:11.334987Z",
     "iopub.status.idle": "2022-03-31T21:01:11.359427Z",
     "shell.execute_reply": "2022-03-31T21:01:11.358802Z",
     "shell.execute_reply.started": "2022-03-31T21:01:11.335220Z"
    }
   },
   "outputs": [],
   "source": [
    "with open ('./cited_papers_20.json','r') as file:\n",
    "    cited_papers = json.load(file)\n",
    "papers = [line.strip() for paper in cited_papers.values() for line in paper ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T10:20:34.734578Z",
     "iopub.status.busy": "2022-03-31T10:20:34.73374Z",
     "iopub.status.idle": "2022-03-31T10:20:40.119359Z",
     "shell.execute_reply": "2022-03-31T10:20:40.118698Z",
     "shell.execute_reply.started": "2022-03-31T10:20:34.734538Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T09:36:02.786005Z",
     "iopub.status.busy": "2022-03-31T09:36:02.785678Z",
     "iopub.status.idle": "2022-03-31T09:36:02.790072Z",
     "shell.execute_reply": "2022-03-31T09:36:02.78927Z",
     "shell.execute_reply.started": "2022-03-31T09:36:02.785967Z"
    }
   },
   "outputs": [],
   "source": [
    "configuration = DistilBertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T10:21:12.869833Z",
     "iopub.status.busy": "2022-03-31T10:21:12.869086Z",
     "iopub.status.idle": "2022-03-31T10:21:29.205196Z",
     "shell.execute_reply": "2022-03-31T10:21:29.204455Z",
     "shell.execute_reply.started": "2022-03-31T10:21:12.869792Z"
    }
   },
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased').to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cosine similarity of citing context and all the cited sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T10:21:29.216542Z",
     "iopub.status.busy": "2022-03-31T10:21:29.215998Z",
     "iopub.status.idle": "2022-03-31T10:21:29.224639Z",
     "shell.execute_reply": "2022-03-31T10:21:29.223879Z",
     "shell.execute_reply.started": "2022-03-31T10:21:29.216504Z"
    }
   },
   "outputs": [],
   "source": [
    "def rank_sentences(context):\n",
    "    context_input = tokenizer(context,return_tensors='pt',max_length=512,truncation=True,padding='max_length')\n",
    "    bert.train(False)\n",
    "    with torch.no_grad():\n",
    "        context_vector = bert(**context_input.to(DEVICE)).last_hidden_state.mean(dim=1)\n",
    "        score_dict = {}\n",
    "        for sent in papers:\n",
    "                cited_input = tokenizer(sent,return_tensors='pt',max_length=512,truncation=True,padding='max_length').to(DEVICE)\n",
    "                cited_vector = bert(**cited_input.to(DEVICE)).last_hidden_state.mean(dim=1)\n",
    "                cosine_similar = F.cosine_similarity(cited_vector,context_vector)\n",
    "                score_dict[sent] = cosine_similar.item()\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T10:21:29.228502Z",
     "iopub.status.busy": "2022-03-31T10:21:29.227764Z",
     "iopub.status.idle": "2022-03-31T11:27:17.697016Z",
     "shell.execute_reply": "2022-03-31T11:27:17.696216Z",
     "shell.execute_reply.started": "2022-03-31T10:21:29.228401Z"
    }
   },
   "outputs": [],
   "source": [
    "citing_score_for_all_sent = {}\n",
    "for file in annotation:\n",
    "    context = annotation[file]['tsa']['citing_context']\n",
    "    print(context)\n",
    "    score_dict = rank_sentences(context)\n",
    "    score_dict = {k:v for k,v in sorted(score_dict.items(),key=lambda x: -x[1])}\n",
    "    citing_score_for_all_sent[context] = score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T11:34:56.547656Z",
     "iopub.status.busy": "2022-03-31T11:34:56.547391Z",
     "iopub.status.idle": "2022-03-31T11:34:56.991675Z",
     "shell.execute_reply": "2022-03-31T11:34:56.990914Z",
     "shell.execute_reply.started": "2022-03-31T11:34:56.547626Z"
    }
   },
   "outputs": [],
   "source": [
    "with open ('context_and_score_bert.json','w') as f:\n",
    "    json.dump(citing_score_for_all_sent,f) \n",
    "\n",
    "# citing_score_for_all_sent = {citing_context:{cited_sentece:cosine_similarity}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Based on manual inspection, we defined a threshold 0.90 for the cosine similarity between the citation context and the cited paper sentences, above which a sentence will be predicted as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in annotation:\n",
    "    context = annotation[file]['tsa']['citing_context']\n",
    "    common_sentences = annotation[file]['common_annotated_sentences']\n",
    "    context_and_cited[context] = common_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('context_and_score_bert.json','r') as file:\n",
    "    context_and_score = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the id and index of context\n",
    "context_index = {}\n",
    "for file in annotation:\n",
    "    context_index[annotation[file]['tsa']['citing_context']] = {'citing_arvix_id':annotation[file]['tsa']['citing_arxiv_id'],'citing_context_list_idx':annotation[file]['tsa']['citing_context_list_idx']}\n",
    "\n",
    "# get the id and index of cited sentences\n",
    "cited_index = {}\n",
    "for id in cited_papers:\n",
    "    for lines in cited_papers[id]:\n",
    "        for line in lines:\n",
    "            cited_index[line.strip()] = {'cited_arxiv_id':id,'cited_sentence_index':lines.index(line)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the result to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = defaultdict(list)\n",
    "for context, score_dict in context_and_score.items():\n",
    "    for sent, score in score_dict.items():\n",
    "        dataframe['citing_arvix_id'].append(context_index[context]['citing_arvix_id'])\n",
    "        dataframe['citing_context_list_idx'].append(context_index[context]['citing_context_list_idx'])\n",
    "        dataframe['citing_context'].append(context)\n",
    "        dataframe['cited_arxiv_id'].append(cited_index[sent]['cited_arxiv_id'])\n",
    "        dataframe['cited_sentence_index'].append(cited_index[sent]['cited_sentence_index'])\n",
    "        dataframe['cited_sentence'].append(sent)\n",
    "        dataframe['cosine_similarity'].append(score)\n",
    "        if score >= 0.90: # set the threshold\n",
    "            dataframe['predicted_label'].append(1)\n",
    "        else:\n",
    "            dataframe['predicted_label'].append(0)\n",
    "        if sent[6:-6] in context_and_cited[context]:\n",
    "            dataframe['real_label'].append(1)\n",
    "        else:\n",
    "            dataframe['real_label'].append(0)\n",
    "\n",
    "df = pd.DataFrame(data=dataframe)\n",
    "# print(df)\n",
    "df.to_csv('result_cosine_similarity_bert_pre.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = df.predicted_label.values\n",
    "real_label = df.real_label.values\n",
    "\n",
    "print(precision_recall_fscore_support(real_label, predicted_label, average='macro'))\n",
    "print(precision_recall_fscore_support(real_label, predicted_label, average='micro'))\n",
    "print(precision_recall_fscore_support(real_label, predicted_label, average=None))\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(real_label, predicted_label, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
