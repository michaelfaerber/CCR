{"1707.09714+60": {"tsa": {"citing_arxiv_id": "1707.09714", "citing_context_list_idx": 60, "citing_context": "This is expected since VB methods tend to underestimate posterior variability MAINCIT . Bayesian Density Estimation Logistic Gaussian process (LGP) priors CIT have been efficiently used as a flexible tool for Bayesian nonparametric density estimation.", "cited_arxiv_id": "1601.00670", "cited_sentence_idx_list": [71], "cited_sentences": ["We do know that variational inference generally underestimates the variance of the posterior density; this is a consequence of its objective function."]}, "syu": {"citing_arxiv_id": "1707.09714", "citing_context_list_idx": 60, "citing_context": "This is expected since VB methods tend to underestimate posterior variability MAINCIT . Bayesian Density Estimation Logistic Gaussian process (LGP) priors CIT have been efficiently used as a flexible tool for Bayesian nonparametric density estimation.", "cited_arxiv_id": "1601.00670", "cited_sentence_idx_list": [71, 72], "cited_sentences": ["We do know that variational inference generally underestimates the variance of the posterior density; this is a consequence of its objective function.", "But, depending on the task at hand, underestimating the variance may be acceptable."]}, "common_annotated_sentences": ["But, depending on the task at hand, underestimating the variance may be acceptable.", "We do know that variational inference generally underestimates the variance of the posterior density; this is a consequence of its objective function."], "common_annotated_sentences_index": [72, 71]}, "1704.05712+97": {"tsa": {"citing_arxiv_id": "1704.05712", "citing_context_list_idx": 97, "citing_context": "Moreover, training with adversarial examples has been applied to mammographic mass segmentation to reduce overfitting MAINCIT . For the methods outlined above, the adversarial perturbation FORMULA depends on the input FORMULA .", "cited_arxiv_id": "1612.05970", "cited_sentence_idx_list": [3, 6, 109], "cited_sentences": ["Inspired by the success of using deep convolutional features for natural image analysis and conditional random fields (CRF) for structural learning, we propose an end-to-end network for mammographic mass segmentation.", "Due to the small size of mammogram datasets, we use adversarial training to control over-fitting.", "The adversarial training works effectively as a regularization to reduce the overfitting."]}, "syu": {"citing_arxiv_id": "1704.05712", "citing_context_list_idx": 97, "citing_context": "Moreover, training with adversarial examples has been applied to mammographic mass segmentation to reduce overfitting MAINCIT . For the methods outlined above, the adversarial perturbation FORMULA depends on the input FORMULA .", "cited_arxiv_id": "1612.05970", "cited_sentence_idx_list": [6, 109, 110], "cited_sentences": ["Due to the small size of mammogram datasets, we use adversarial training to control over-fitting.", "The adversarial training works effectively as a regularization to reduce the overfitting.", "We believe that the overfitting is mainly caused by the small training set size and we strongly support the creation of a large mammographic analysis dataset to accelerate mammogram analysis research."]}, "common_annotated_sentences": ["The adversarial training works effectively as a regularization to reduce the overfitting.", "Due to the small size of mammogram datasets, we use adversarial training to control over-fitting.", "We believe that the overfitting is mainly caused by the small training set size and we strongly support the creation of a large mammographic analysis dataset to accelerate mammogram analysis research.", "Inspired by the success of using deep convolutional features for natural image analysis and conditional random fields (CRF) for structural learning, we propose an end-to-end network for mammographic mass segmentation."], "common_annotated_sentences_index": [110, 3, 109, 6]}, "1706.09077+111": {"tsa": {"citing_arxiv_id": "1706.09077", "citing_context_list_idx": 111, "citing_context": "Deep Networks for Image Super-resolution According to MAINCIT , while employing a CNN for restoration tasks (like SR and denoising), pooling or subsampling may be counter-productive as important image details may be discarded. Hence pooling layers are usually avoided in SR tasks which again has its downside; each additional convolutional layer means a new weight layer and hence more parameters with the consequences in the form of overfitting and too huge a model to store and retrieve.", "cited_arxiv_id": "1511.04491", "cited_sentence_idx_list": [10, 16, 17], "cited_sentences": ["As SR is an ill-posed inverse problem, collecting and analyzing more neighbor pixels can possibly give more clues on what may be lost by downsampling.", "For image restoration problems such as super-resolution and denoising, image details are very important.", "Therefore, most deep-learning approaches for such problems do not use pooling."]}, "syu": {"citing_arxiv_id": "1706.09077", "citing_context_list_idx": 111, "citing_context": "Deep Networks for Image Super-resolution According to MAINCIT , while employing a CNN for restoration tasks (like SR and denoising), pooling or subsampling may be counter-productive as important image details may be discarded. Hence pooling layers are usually avoided in SR tasks which again has its downside; each additional convolutional layer means a new weight layer and hence more parameters with the consequences in the form of overfitting and too huge a model to store and retrieve.", "cited_arxiv_id": "1511.04491", "cited_sentence_idx_list": [16, 17, 248], "cited_sentences": ["For image restoration problems such as super-resolution and denoising, image details are very important.", "Therefore, most deep-learning approaches for such problems do not use pooling.", "We believe our approach is readily applicable to other image restoration problems such as denoising and compression artifact removal."]}, "common_annotated_sentences": ["We believe our approach is readily applicable to other image restoration problems such as denoising and compression artifact removal.", "Therefore, most deep-learning approaches for such problems do not use pooling.", "For image restoration problems such as super-resolution and denoising, image details are very important.", "As SR is an ill-posed inverse problem, collecting and analyzing more neighbor pixels can possibly give more clues on what may be lost by downsampling."], "common_annotated_sentences_index": [16, 17, 10, 248]}, "1706.02379+126": {"tsa": {"citing_arxiv_id": "1706.02379", "citing_context_list_idx": 126, "citing_context": "Experiments show that networks using 16-bit fixed-point representations with stochastic rounding can deliver results nearly identical to 32-bit floating-point computations CIT , while lowering the precision down to 3-bit fixed-point often results in a significant performance degradation MAINCIT . Bayesian learning has also been applied to train binary networks CIT , CIT .", "cited_arxiv_id": "1603.01025", "cited_sentence_idx_list": [2, 3], "cited_sentences": ["For example, it is now well-known that the arithmetic operations of deep networks can be encoded down to 8-bit fixed-point without significant deterioration in performance.", "However, further reduction in precision down to as low as 3-bit fixed-point results in significant losses in performance."]}, "syu": {"citing_arxiv_id": "1706.02379", "citing_context_list_idx": 126, "citing_context": "Experiments show that networks using 16-bit fixed-point representations with stochastic rounding can deliver results nearly identical to 32-bit floating-point computations CIT , while lowering the precision down to 3-bit fixed-point often results in a significant performance degradation MAINCIT . Bayesian learning has also been applied to train binary networks CIT , CIT .", "cited_arxiv_id": "1603.01025", "cited_sentence_idx_list": [3], "cited_sentences": ["However, further reduction in precision down to as low as 3-bit fixed-point results in significant losses in performance."]}, "common_annotated_sentences": ["However, further reduction in precision down to as low as 3-bit fixed-point results in significant losses in performance.", "For example, it is now well-known that the arithmetic operations of deep networks can be encoded down to 8-bit fixed-point without significant deterioration in performance."], "common_annotated_sentences_index": [2, 3]}, "2001.09233+133": {"tsa": {"citing_arxiv_id": "2001.09233", "citing_context_list_idx": 133, "citing_context": "Some authors have argued that using separate thresholds in the interest of balancing predictive equity in itself falls short of fairness by treating individuals with similar risk profiles in different ways MAINCIT . However, concepts of fairness through unawareness have been consistently demonstrated to be misguided CIT , CIT , CIT , CIT , CIT , CIT , and any process that seeks to balance the dual goals of equity and efficiency will face an inherent trade-off between these objectives, even where it is obscured by the process involved.", "cited_arxiv_id": "1701.08230", "cited_sentence_idx_list": [11, 12, 13, 28, 30, 31, 339, 340], "cited_sentences": ["We show that for several past definitions of fairness, the optimal algorithms that result require detaining defendants above race-specific risk thresholds.", "We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants.", "The unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race.", "We show that for several past definitions of fairness, the optimal algorithms that result require applying multiple, race-specific thresholds to individuals' risk scores.", "We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants.", "This safety-maximizing rule thus satisfies one important understanding of equality: that all individuals are held to the same standard, irrespective of race.", "A single-threshold rule thus maximizes public safety while satisfying a core constitutional law rule, bolstering the case in its favor.", "To some extent, concerns embodied by past fairness definitions can be addressed while still adopting a single-threshold rule."]}, "syu": {"citing_arxiv_id": "2001.09233", "citing_context_list_idx": 133, "citing_context": "Some authors have argued that using separate thresholds in the interest of balancing predictive equity in itself falls short of fairness by treating individuals with similar risk profiles in different ways MAINCIT . However, concepts of fairness through unawareness have been consistently demonstrated to be misguided CIT , CIT , CIT , CIT , CIT , CIT , and any process that seeks to balance the dual goals of equity and efficiency will face an inherent trade-off between these objectives, even where it is obscured by the process involved.", "cited_arxiv_id": "1701.08230", "cited_sentence_idx_list": [11, 12, 13, 28, 30, 31], "cited_sentences": ["We show that for several past definitions of fairness, the optimal algorithms that result require detaining defendants above race-specific risk thresholds.", "We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants.", "The unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race.", "We show that for several past definitions of fairness, the optimal algorithms that result require applying multiple, race-specific thresholds to individuals' risk scores.", "We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants.", "This safety-maximizing rule thus satisfies one important understanding of equality: that all individuals are held to the same standard, irrespective of race."]}, "common_annotated_sentences": ["The unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race.", "We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants.", "We show that for several past definitions of fairness, the optimal algorithms that result require applying multiple, race-specific thresholds to individuals' risk scores.", "To some extent, concerns embodied by past fairness definitions can be addressed while still adopting a single-threshold rule.", "We show that for several past definitions of fairness, the optimal algorithms that result require detaining defendants above race-specific risk thresholds.", "This safety-maximizing rule thus satisfies one important understanding of equality: that all individuals are held to the same standard, irrespective of race.", "A single-threshold rule thus maximizes public safety while satisfying a core constitutional law rule, bolstering the case in its favor."], "common_annotated_sentences_index": [11, 12, 13, 339, 340, 28, 30, 31]}, "2003.03546+135": {"tsa": {"citing_arxiv_id": "2003.03546", "citing_context_list_idx": 135, "citing_context": "Alternative solutions include the use of Bayesian NNs, as there is evidence that uncertainty measures can predict crashes up to five seconds in advance MAINCIT . CIT propose propagating the uncertainty over the network to achieve a safer driving style.", "cited_arxiv_id": "1811.06817", "cited_sentence_idx_list": [6, 35, 198, 199], "cited_sentences": ["We propose evaluation techniques for the uncertainty on two separate architectures which use the uncertainty to predict crashes up to five seconds in advance.", "We present preliminary results that show significant changes in uncertainty, specifically mutual information, up to five seconds before crashes.", "Mutual information was once again the strongest indicator of incorrect behaviour, in this case meaning the car crashing.", "The mutual information in this graph peaks at around 27 frames, or 4.5 seconds, before the crash."]}, "syu": {"citing_arxiv_id": "2003.03546", "citing_context_list_idx": 135, "citing_context": "Alternative solutions include the use of Bayesian NNs, as there is evidence that uncertainty measures can predict crashes up to five seconds in advance MAINCIT . CIT propose propagating the uncertainty over the network to achieve a safer driving style.", "cited_arxiv_id": "1811.06817", "cited_sentence_idx_list": [6, 35], "cited_sentences": ["We propose evaluation techniques for the uncertainty on two separate architectures which use the uncertainty to predict crashes up to five seconds in advance.", "We present preliminary results that show significant changes in uncertainty, specifically mutual information, up to five seconds before crashes."]}, "common_annotated_sentences": ["The mutual information in this graph peaks at around 27 frames, or 4.5 seconds, before the crash.", "Mutual information was once again the strongest indicator of incorrect behaviour, in this case meaning the car crashing.", "We propose evaluation techniques for the uncertainty on two separate architectures which use the uncertainty to predict crashes up to five seconds in advance.", "We present preliminary results that show significant changes in uncertainty, specifically mutual information, up to five seconds before crashes."], "common_annotated_sentences_index": [35, 198, 6, 199]}, "1809.04663+137": {"tsa": {"citing_arxiv_id": "1809.04663", "citing_context_list_idx": 137, "citing_context": "Formally, the demographic parity criterion may be expressed as FORMULA However, optimizing for demographic parity is of limited use for clinical risk prediction, because doing so may preclude the model from considering relevant clinical features associated with the sensitive attribute and the outcome, thus decreasing the performance of the model for all groups MAINCIT . Another related metric is equality of odds CIT , which stipulates that the prediction FORMULA be conditionally independent of FORMULA , given the true label FORMULA .", "cited_arxiv_id": "1609.05807", "cited_sentence_idx_list": [377, 378, 380, 381, 382], "cited_sentences": ["In this work we have formalized three fundamental conditions for risk assignments to individuals, each of which has been proposed as a basic measure of what it means for the risk assignment to be fair.", "Our main results show that except in highly constrained special cases, it is not possible to satisfy these three constraints simultaneously; and moreover, a version of this fact holds in an approximate sense as well.", "To take one simple example, suppose we want to determine the risk that a person is a carrier for a disease FORMULA , and suppose that a higher fraction of women than men are carriers.", "Then our results imply that in any test designed to estimate the probability that someone is a carrier of FORMULA , at least one of the following undesirable properties must hold: (a) the test's probability estimates are systematically skewed upward or downward for at least one gender; or (b) the test assigns a higher average risk estimate to healthy people (non-carriers) in one gender than the other; or (c) the test assigns a higher average risk estimate to carriers of the disease in one gender than the other.", "The point is that this trade-off among (a), (b), and (c) is not a fact about medicine; it is simply a fact about risk estimates when the base rates differ between two groups."]}, "syu": {"citing_arxiv_id": "1809.04663", "citing_context_list_idx": 137, "citing_context": "Formally, the demographic parity criterion may be expressed as FORMULA However, optimizing for demographic parity is of limited use for clinical risk prediction, because doing so may preclude the model from considering relevant clinical features associated with the sensitive attribute and the outcome, thus decreasing the performance of the model for all groups MAINCIT . Another related metric is equality of odds CIT , which stipulates that the prediction FORMULA be conditionally independent of FORMULA , given the true label FORMULA .", "cited_arxiv_id": "1609.05807", "cited_sentence_idx_list": [19, 20, 21], "cited_sentences": ["A third domain, again quite different from the previous two, is medical testing and diagnosis.", "Doctors making decisions about a patient's treatment may rely on tests providing probability estimates for different diseases and conditions.", "Here too we can ask whether such decision-making is being applied uniformly across different groups of patients {{cite:4ff540c0-f2b0-4da2-beeb-4fd184113692}}, {{cite:dcdc455f-92e7-49ef-9b17-42267468435a}}, and in particular how medical tests may play a differential role for conditions that vary widely in frequency between these groups."]}, "common_annotated_sentences": ["Here too we can ask whether such decision-making is being applied uniformly across different groups of patients {{cite:4ff540c0-f2b0-4da2-beeb-4fd184113692}}, {{cite:dcdc455f-92e7-49ef-9b17-42267468435a}}, and in particular how medical tests may play a differential role for conditions that vary widely in frequency between these groups.", "Our main results show that except in highly constrained special cases, it is not possible to satisfy these three constraints simultaneously; and moreover, a version of this fact holds in an approximate sense as well.", "A third domain, again quite different from the previous two, is medical testing and diagnosis.", "Doctors making decisions about a patient's treatment may rely on tests providing probability estimates for different diseases and conditions.", "In this work we have formalized three fundamental conditions for risk assignments to individuals, each of which has been proposed as a basic measure of what it means for the risk assignment to be fair.", "Then our results imply that in any test designed to estimate the probability that someone is a carrier of FORMULA , at least one of the following undesirable properties must hold: (a) the test's probability estimates are systematically skewed upward or downward for at least one gender; or (b) the test assigns a higher average risk estimate to healthy people (non-carriers) in one gender than the other; or (c) the test assigns a higher average risk estimate to carriers of the disease in one gender than the other.", "The point is that this trade-off among (a), (b), and (c) is not a fact about medicine; it is simply a fact about risk estimates when the base rates differ between two groups.", "To take one simple example, suppose we want to determine the risk that a person is a carrier for a disease FORMULA , and suppose that a higher fraction of women than men are carriers."], "common_annotated_sentences_index": [19, 20, 21, 377, 378, 380, 381, 382]}, "2005.01259+142": {"tsa": {"citing_arxiv_id": "2005.01259", "citing_context_list_idx": 142, "citing_context": "Dropout CIT as a way of regularization has been shown effective in deep learning models, and MAINCIT has successfully applied dropout-like technique in LSTM: the use of DropConnect CIT is applied on the four hidden-to-hidden matrices, preventing overfitting from occurring on the recurrent weights. Reinforcement Learning Reinforcement learning is applied to the best performing encoder in Section to prune noisy text, which can lead to comparable or even better performance, as many text segments in these clinical notes are found to be irrelevant to this task.", "cited_arxiv_id": "1708.02182", "cited_sentence_idx_list": [103, 203, 204, 207], "cited_sentences": ["While we propose using DropConnect rather than variational dropout to regularize the hidden-to-hidden transition within an RNN, we use variational dropout for all other dropout operations, specifically using the same dropout mask for all inputs and outputs of the LSTM within a given forward and backward pass.", "In this work, we discuss regularization and optimization strategies for neural language models.", "We propose the weight-dropped LSTM, a strategy that uses a DropConnect mask on the hidden-to-hidden weight matrices, as a means to prevent overfitting across the recurrent connections.", "Our models outperform custom-built RNN cells and complex regularization strategies that preclude the possibility of using optimized libraries such as the NVIDIA cuDNN LSTM."]}, "syu": {"citing_arxiv_id": "2005.01259", "citing_context_list_idx": 142, "citing_context": "Dropout CIT as a way of regularization has been shown effective in deep learning models, and MAINCIT has successfully applied dropout-like technique in LSTM: the use of DropConnect CIT is applied on the four hidden-to-hidden matrices, preventing overfitting from occurring on the recurrent weights. Reinforcement Learning Reinforcement learning is applied to the best performing encoder in Section to prune noisy text, which can lead to comparable or even better performance, as many text segments in these clinical notes are found to be irrelevant to this task.", "cited_arxiv_id": "1708.02182", "cited_sentence_idx_list": [3, 22, 45, 204], "cited_sentences": ["We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization.", "The weight-dropped LSTM applies recurrent regularization through a DropConnect mask on the hidden-to-hidden recurrent weights.", "By performing DropConnect on the hidden-to-hidden weight matrices FORMULA  within the LSTM, we can prevent overfitting from occurring on the recurrent connections of the LSTM.", "We propose the weight-dropped LSTM, a strategy that uses a DropConnect mask on the hidden-to-hidden weight matrices, as a means to prevent overfitting across the recurrent connections."]}, "common_annotated_sentences": ["In this work, we discuss regularization and optimization strategies for neural language models.", "By performing DropConnect on the hidden-to-hidden weight matrices FORMULA  within the LSTM, we can prevent overfitting from occurring on the recurrent connections of the LSTM.", "We propose the weight-dropped LSTM, a strategy that uses a DropConnect mask on the hidden-to-hidden weight matrices, as a means to prevent overfitting across the recurrent connections.", "While we propose using DropConnect rather than variational dropout to regularize the hidden-to-hidden transition within an RNN, we use variational dropout for all other dropout operations, specifically using the same dropout mask for all inputs and outputs of the LSTM within a given forward and backward pass.", "The weight-dropped LSTM applies recurrent regularization through a DropConnect mask on the hidden-to-hidden recurrent weights.", "Our models outperform custom-built RNN cells and complex regularization strategies that preclude the possibility of using optimized libraries such as the NVIDIA cuDNN LSTM.", "We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization."], "common_annotated_sentences_index": [3, 103, 203, 204, 45, 207, 22]}, "1811.07275+160": {"tsa": {"citing_arxiv_id": "1811.07275", "citing_context_list_idx": 160, "citing_context": "Orthogonal initialization is common practice for Recurrent Neural Networks due to their increased sensitivity to initial conditions MAINCIT , but it has somewhat fallen out of favor for ConvNets. These factors shape our motivation for encouraging orthogonality of features in the ConvNet and form the basis of our ranking criteria.", "cited_arxiv_id": "1702.00071", "cited_sentence_idx_list": [1, 17, 86, 158, 159, 160, 161, 162, 221], "cited_sentences": ["On orthogonality and learning RNNs with long term dependencies [ On orthogonality and learning recurrent networks with long term dependencies Eugene Vorontsovpoly,mila Chiheb Trabelsipoly,mila Samuel Kadourypoly,chum Chris Palpoly,mila poly\u00c9cole Polytechnique de Montr\u00e9al, Montr\u00e9al, Canada milaMontreal Institute for Learning Algorithms, Montr\u00e9al, Canada chumCHUM Research Center, Montr\u00e9al, Canada Eugene Vorontsoveugene.vorontsov@gmail.com ] It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies.", "{{cite:e670dc65-c105-4ab5-905e-3ee1e66d96fa}} and \u00a0{{cite:4bda33e7-5f2b-485b-8766-03c2472b3755}} have respectively shown that identity initialization and orthogonal initialization can be beneficial.", "We confirm that orthogonal initialization is useful as noted in {{cite:4bda33e7-5f2b-485b-8766-03c2472b3755}}, and we show that although strict orthogonality guarantees stable gradient norm, loosening orthogonality constraints can increase the rate of gradient descent convergence.", "An LSTM outperformed the RNNs in both tasks; nevertheless, RNNs with hidden to hidden transitions initialized as orthogonal matrices performed admirably without a memory component and without all of the additional parameters associated with gates.", "Indeed, orthogonally initialized RNNs performed almost on par with the LSTM in the permuted sequential MNIST task which presents longer distance dependencies than the ordered task.", "Although the optimal margin appears to be 0.1, RNNs with large margins perform almost identically to an RNN without a margin, as long as the transition matrix is initialized as orthogonal.", "On these tasks, orthogonal initialization appears to significantly outperform Glorot normal initialization {{cite:b1d13940-f837-4bff-b96e-4a6387da78f4}} or initializing the matrix as identity.", "It is interesting to note that for the MNIST tasks, orthogonal initialization appears useful while orthogonality constraints appear mainly detrimental.", "Our experiments indicate that while orthogonal initialization may be beneficial, maintaining hard constraints on orthogonality can be detrimental."]}, "syu": {"citing_arxiv_id": "1811.07275", "citing_context_list_idx": 160, "citing_context": "Orthogonal initialization is common practice for Recurrent Neural Networks due to their increased sensitivity to initial conditions MAINCIT , but it has somewhat fallen out of favor for ConvNets. These factors shape our motivation for encouraging orthogonality of features in the ConvNet and form the basis of our ranking criteria.", "cited_arxiv_id": "1702.00071", "cited_sentence_idx_list": [86], "cited_sentences": ["We confirm that orthogonal initialization is useful as noted in {{cite:4bda33e7-5f2b-485b-8766-03c2472b3755}}, and we show that although strict orthogonality guarantees stable gradient norm, loosening orthogonality constraints can increase the rate of gradient descent convergence."]}, "common_annotated_sentences": ["Although the optimal margin appears to be 0.1, RNNs with large margins perform almost identically to an RNN without a margin, as long as the transition matrix is initialized as orthogonal.", "On orthogonality and learning RNNs with long term dependencies [ On orthogonality and learning recurrent networks with long term dependencies Eugene Vorontsovpoly,mila Chiheb Trabelsipoly,mila Samuel Kadourypoly,chum Chris Palpoly,mila poly\u00c9cole Polytechnique de Montr\u00e9al, Montr\u00e9al, Canada milaMontreal Institute for Learning Algorithms, Montr\u00e9al, Canada chumCHUM Research Center, Montr\u00e9al, Canada Eugene Vorontsoveugene.vorontsov@gmail.com ] It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies.", "It is interesting to note that for the MNIST tasks, orthogonal initialization appears useful while orthogonality constraints appear mainly detrimental.", "We confirm that orthogonal initialization is useful as noted in {{cite:4bda33e7-5f2b-485b-8766-03c2472b3755}}, and we show that although strict orthogonality guarantees stable gradient norm, loosening orthogonality constraints can increase the rate of gradient descent convergence.", "Our experiments indicate that while orthogonal initialization may be beneficial, maintaining hard constraints on orthogonality can be detrimental.", "An LSTM outperformed the RNNs in both tasks; nevertheless, RNNs with hidden to hidden transitions initialized as orthogonal matrices performed admirably without a memory component and without all of the additional parameters associated with gates.", "Indeed, orthogonally initialized RNNs performed almost on par with the LSTM in the permuted sequential MNIST task which presents longer distance dependencies than the ordered task.", "{{cite:e670dc65-c105-4ab5-905e-3ee1e66d96fa}} and \u00a0{{cite:4bda33e7-5f2b-485b-8766-03c2472b3755}} have respectively shown that identity initialization and orthogonal initialization can be beneficial.", "On these tasks, orthogonal initialization appears to significantly outperform Glorot normal initialization {{cite:b1d13940-f837-4bff-b96e-4a6387da78f4}} or initializing the matrix as identity."], "common_annotated_sentences_index": [160, 1, 161, 162, 17, 86, 221, 158, 159]}, "1808.03827+171": {"tsa": {"citing_arxiv_id": "1808.03827", "citing_context_list_idx": 171, "citing_context": "Biases are especially troubling when applying machine learning because the model might identify and exacerbate biases in a feedback loop MAINCIT . However, some select clinical tasks do benefit from knowing the patient's race (e.g. when there are differences in recommended care by genetic makeup).", "cited_arxiv_id": "1706.09847", "cited_sentence_idx_list": [9, 10, 11, 12, 307, 308], "cited_sentences": ["Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated.", "Such systems have been empirically shown to be susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.", "In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned.", "Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas.", "In this paper we show that urn models can be used to formally model predictive policing as well as indicate remedies for problems with feedback.", "We demonstrate this both formally and empirically."]}, "syu": {"citing_arxiv_id": "1808.03827", "citing_context_list_idx": 171, "citing_context": "Biases are especially troubling when applying machine learning because the model might identify and exacerbate biases in a feedback loop MAINCIT . However, some select clinical tasks do benefit from knowing the patient's race (e.g. when there are differences in recommended care by genetic makeup).", "cited_arxiv_id": "1706.09847", "cited_sentence_idx_list": [27, 311, 312], "cited_sentences": ["Since such discovered incidents only occur in neighborhoods that police have been sent to by the predictive policing algorithm itself, there is the potential for this sampling bias to be compounded, causing a runaway feedback loop.", "They also indicate exactly how the problem of runaway feedback can be exacerbated: specifically as crime rates vary between regions and as the model relies more and more on discovered incident reports.", "Our results also indicate that if crime rates are more or less the same between regions, then the problem of feedback is much less, and it might be possible to generate reasonable predictions without explicitly countering feedback loops (though the results will still be inaccurate)."]}, "common_annotated_sentences": ["In this paper we show that urn models can be used to formally model predictive policing as well as indicate remedies for problems with feedback.", "Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas.", "Since such discovered incidents only occur in neighborhoods that police have been sent to by the predictive policing algorithm itself, there is the potential for this sampling bias to be compounded, causing a runaway feedback loop.", "Our results also indicate that if crime rates are more or less the same between regions, then the problem of feedback is much less, and it might be possible to generate reasonable predictions without explicitly countering feedback loops (though the results will still be inaccurate).", "In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned.", "Such systems have been empirically shown to be susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate.", "Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated.", "We demonstrate this both formally and empirically.", "They also indicate exactly how the problem of runaway feedback can be exacerbated: specifically as crime rates vary between regions and as the model relies more and more on discovered incident reports."], "common_annotated_sentences_index": [9, 10, 11, 12, 307, 308, 311, 312, 27]}, "2007.11731+183": {"tsa": {"citing_arxiv_id": "2007.11731", "citing_context_list_idx": 183, "citing_context": "Major progress has been made in image captioning MAINCIT . An encoder-decoder model is often considered, where Convolutional Neural Networks (CNNs) are used to extract global image features, and Recurrent Neural Networks (RNNs) are used to decode the features into sentences CIT , CIT , CIT , CIT , CIT , CIT , CIT , CIT .", "cited_arxiv_id": "1810.04020", "cited_sentence_idx_list": [812, 820, 821, 822, 823, 824, 833], "cited_sentences": ["Many deep learning-based methods have been proposed for generating automatic image captions in the recent years.", "Attention-based image captioning methods focus on different salient parts of the image and achieve better performance than encoder-decoder architecture-based methods.", "Semantic concept-based image captioning methods selectively focus on different parts of the image and can generate semantically rich captions.", "Dense image captioning methods can generate region based image captions.", "Stylized image captions express various emotions such as romance, pride, and shame.", "GAN and RL based image captioning methods can generate diverse and multiple captions.", "Although success has been achieved in recent years, there is still a large scope for improvement."]}, "syu": {"citing_arxiv_id": "2007.11731", "citing_context_list_idx": 183, "citing_context": "Major progress has been made in image captioning MAINCIT . An encoder-decoder model is often considered, where Convolutional Neural Networks (CNNs) are used to extract global image features, and Recurrent Neural Networks (RNNs) are used to decode the features into sentences CIT , CIT , CIT , CIT , CIT , CIT , CIT , CIT .", "cited_arxiv_id": "1810.04020", "cited_sentence_idx_list": [10, 39, 40, 850], "cited_sentences": ["Deep learning-based techniques are capable of handling the complexities and challenges of image captioning.", "In the last 5 years, a large number of articles have been published on image captioning with deep machine learning being popularly used.", "Deep learning algorithms can handle complexities and challenges of image captioning quite well.", "Although deep learning-based image captioning methods have achieved a remarkable progress in recent years, a robust image captioning method that is able to generate high quality captions for nearly all images is yet to be achieved."]}, "common_annotated_sentences": ["Although success has been achieved in recent years, there is still a large scope for improvement.", "Semantic concept-based image captioning methods selectively focus on different parts of the image and can generate semantically rich captions.", "Attention-based image captioning methods focus on different salient parts of the image and achieve better performance than encoder-decoder architecture-based methods.", "Deep learning-based techniques are capable of handling the complexities and challenges of image captioning.", "Deep learning algorithms can handle complexities and challenges of image captioning quite well.", "Dense image captioning methods can generate region based image captions.", "Although deep learning-based image captioning methods have achieved a remarkable progress in recent years, a robust image captioning method that is able to generate high quality captions for nearly all images is yet to be achieved.", "In the last 5 years, a large number of articles have been published on image captioning with deep machine learning being popularly used.", "Many deep learning-based methods have been proposed for generating automatic image captions in the recent years.", "GAN and RL based image captioning methods can generate diverse and multiple captions.", "Stylized image captions express various emotions such as romance, pride, and shame."], "common_annotated_sentences_index": [833, 39, 40, 10, 812, 850, 820, 821, 822, 823, 824]}, "1606.03059+227": {"tsa": {"citing_arxiv_id": "1606.03059", "citing_context_list_idx": 227, "citing_context": "The BHV distance can be computed in polynomial time MAINCIT , in contrast with other distances on tree spaces which are typically NP-hard to compute. BHV space has been previously used to extend classical statistical methods for phylogenetic inference.", "cited_arxiv_id": "0907.3942", "cited_sentence_idx_list": [6, 8, 32, 35, 36, 338], "cited_sentences": ["The geodesic distance measure between two phylogenetic trees with edge lengths is the length of the shortest path between them in the continuous tree space introduced by Billera, Holmes, and Vogtmann.", "An important open problem is to find a polynomial time algorithm for finding geodesics in tree space.", "This paper presents the first polynomial-time method for computing geodesic distances \u2014 and the associated geodesics \u2014 between trees in tree space.", "By restricting consideration to the orthants intersecting the geodesic, the algorithm makes only a polynomial number of path transformations.", "Each new orthant is identified by finding a weighted vertex cover in a specially constructed bipartite graph, which also is a polynomial time problem.", "Conclusion This paper presents the first polynomial time algorithm for finding geodesics between phylogenetic trees in tree space, as well as further characterizing properties of geodesics."]}, "syu": {"citing_arxiv_id": "1606.03059", "citing_context_list_idx": 227, "citing_context": "The BHV distance can be computed in polynomial time MAINCIT , in contrast with other distances on tree spaces which are typically NP-hard to compute. BHV space has been previously used to extend classical statistical methods for phylogenetic inference.", "cited_arxiv_id": "0907.3942", "cited_sentence_idx_list": [8, 30], "cited_sentences": ["An important open problem is to find a polynomial time algorithm for finding geodesics in tree space.", "Currently there are no known polynomial-time algorithms to find tree space geodesics, although a polynomial time FORMULA -approximation of the geodesic distance was given by Amenta et al. {{cite:73c6b6b9-cf67-4a84-a9ac-b014557db05a}}."]}, "common_annotated_sentences": ["This paper presents the first polynomial-time method for computing geodesic distances \u2014 and the associated geodesics \u2014 between trees in tree space.", "Conclusion This paper presents the first polynomial time algorithm for finding geodesics between phylogenetic trees in tree space, as well as further characterizing properties of geodesics.", "The geodesic distance measure between two phylogenetic trees with edge lengths is the length of the shortest path between them in the continuous tree space introduced by Billera, Holmes, and Vogtmann.", "By restricting consideration to the orthants intersecting the geodesic, the algorithm makes only a polynomial number of path transformations.", "An important open problem is to find a polynomial time algorithm for finding geodesics in tree space.", "Each new orthant is identified by finding a weighted vertex cover in a specially constructed bipartite graph, which also is a polynomial time problem.", "Currently there are no known polynomial-time algorithms to find tree space geodesics, although a polynomial time FORMULA -approximation of the geodesic distance was given by Amenta et al. {{cite:73c6b6b9-cf67-4a84-a9ac-b014557db05a}}."], "common_annotated_sentences_index": [32, 35, 36, 6, 8, 338, 30]}, "1903.03927+2": {"tsa": {"citing_arxiv_id": "1903.03927", "citing_context_list_idx": 2, "citing_context": "Graph cuts optimization was applied to the output of a hierarchical two-stage classifier, which was trained to identify the cartilage and bone voxels MAINCIT . As the underlying multi-label graph cuts jointly consider independent classifier outputs for cartilage, bone, and background, label-conflict-resolution may be challenging in regions with multiple labels.", "cited_arxiv_id": "1307.2965", "cited_sentence_idx_list": [26, 63, 71, 102], "cited_sentences": ["In this paper, we present a fully automatic learning-based voxel classification method for cartilage segmentation.", "Iterative Semantic Context Forests In this paper, we present a multi-pass iterative classification method to automatically exploit the semantic context for multiple object segmentation problems.", "Post-processing by Graph Cuts Optimization After the classification, we finally use the probabilities of being the background and the three cartilages to construct the energy functions and perform multi-label graph cuts {{cite:ee6554ca-9a54-4f9d-bb30-47ca8e0e4f02}} to refine the segmentation with smoothness constraints.", "FIGURE    Conclusion   We have presented a new approach to segment the three knee cartilages in 3-D MR images, which effectively exploits the semantic context information in the knee joint."]}, "syu": {"citing_arxiv_id": "1903.03927", "citing_context_list_idx": 2, "citing_context": "Graph cuts optimization was applied to the output of a hierarchical two-stage classifier, which was trained to identify the cartilage and bone voxels MAINCIT . As the underlying multi-label graph cuts jointly consider independent classifier outputs for cartilage, bone, and background, label-conflict-resolution may be challenging in regions with multiple labels.", "cited_arxiv_id": "1307.2965", "cited_sentence_idx_list": [6, 7, 8], "cited_sentences": ["In this paper, we present an iterative multi-class learning method to segment the femoral, tibial and patellar cartilage simultaneously, which effectively exploits the spatial contextual constraints between bone and cartilage, and also between different cartilages.", "First, based on the fact that the cartilage grows in only certain area of the corresponding bone surface, we extract the distance features of not only to the surface of the bone, but more informatively, to the densely registered anatomical landmarks on the bone surface.", "Second, we introduce a set of iterative discriminative classifiers that at each iteration, probability comparison features are constructed from the class confidence maps derived by previously learned classifiers."]}, "common_annotated_sentences": ["Second, we introduce a set of iterative discriminative classifiers that at each iteration, probability comparison features are constructed from the class confidence maps derived by previously learned classifiers.", "Iterative Semantic Context Forests In this paper, we present a multi-pass iterative classification method to automatically exploit the semantic context for multiple object segmentation problems.", "In this paper, we present a fully automatic learning-based voxel classification method for cartilage segmentation.", "Post-processing by Graph Cuts Optimization After the classification, we finally use the probabilities of being the background and the three cartilages to construct the energy functions and perform multi-label graph cuts {{cite:ee6554ca-9a54-4f9d-bb30-47ca8e0e4f02}} to refine the segmentation with smoothness constraints.", "First, based on the fact that the cartilage grows in only certain area of the corresponding bone surface, we extract the distance features of not only to the surface of the bone, but more informatively, to the densely registered anatomical landmarks on the bone surface.", "In this paper, we present an iterative multi-class learning method to segment the femoral, tibial and patellar cartilage simultaneously, which effectively exploits the spatial contextual constraints between bone and cartilage, and also between different cartilages.", "FIGURE    Conclusion   We have presented a new approach to segment the three knee cartilages in 3-D MR images, which effectively exploits the semantic context information in the knee joint."], "common_annotated_sentences_index": [102, 6, 71, 7, 8, 26, 63]}, "1901.03495+8": {"tsa": {"citing_arxiv_id": "1901.03495", "citing_context_list_idx": 8, "citing_context": "There are some approaches using message passing among features for segmentation CIT , pose estimation MAINCIT and object detection CIT . These designs are based on backbone networks, and the FishNet is a backbone network complementary to them.", "cited_arxiv_id": "1611.00468", "cited_sentence_idx_list": [10, 11, 33, 34, 41, 205, 207], "cited_sentences": ["In this paper, we propose a CRF-CNN framework which can simultaneously model structural information in both output and hidden feature layers in a probabilistic way, and it is applied to human pose estimation.", "A message passing scheme is proposed, so that in various layers each body joint receives messages from all the others in an efficient way.", "In contrast, our proposed full CRF-CNN model takes feature-output, output-output, and feature-feature relationships into consideration, which is novel in pose estimation.", "It also facilitates us in borrowing the idea behind the sum-product algorithm and developing a message passing scheme so that each body joint receives messages from all the others in an efficient way by saving intermediate messages.", "(2) Motivated by the efficient algorithm for marginalization on tree structures, we provide a message passing scheme for our CRF-CNN so that every vertex receives messages from all the others in an efficient way.", "Conclusion We propose to use CRF for modeling structured features and structured human body part configurations.", "The efficient sum-product algorithm in the probabilistic model guides us in using an efficient message passing approach so that each vertex receives messages from other nodes in a more efficient way."]}, "syu": {"citing_arxiv_id": "1901.03495", "citing_context_list_idx": 8, "citing_context": "There are some approaches using message passing among features for segmentation CIT , pose estimation MAINCIT and object detection CIT . These designs are based on backbone networks, and the FishNet is a backbone network complementary to them.", "cited_arxiv_id": "1611.00468", "cited_sentence_idx_list": [10, 20, 33], "cited_sentences": ["In this paper, we propose a CRF-CNN framework which can simultaneously model structural information in both output and hidden feature layers in a probabilistic way, and it is applied to human pose estimation.", "Human pose estimation is to estimate body joint locations from 2D images, which could be applied to assist other tasks such as {{cite:71b7254e-7448-46d2-96a9-124ee9d4efa1}}, {{cite:15566a9d-5693-429c-b58e-864e2ba77bea}}, {{cite:7402a231-5b79-43e5-93ab-a0b9978ac104}} The very first attempt adopting CNN for human pose estimation is DeepPose {{cite:62c83b7c-f2e5-42a5-b3c1-b9788087e79c}}.", "In contrast, our proposed full CRF-CNN model takes feature-output, output-output, and feature-feature relationships into consideration, which is novel in pose estimation."]}, "common_annotated_sentences": ["The efficient sum-product algorithm in the probabilistic model guides us in using an efficient message passing approach so that each vertex receives messages from other nodes in a more efficient way.", "(2) Motivated by the efficient algorithm for marginalization on tree structures, we provide a message passing scheme for our CRF-CNN so that every vertex receives messages from all the others in an efficient way.", "A message passing scheme is proposed, so that in various layers each body joint receives messages from all the others in an efficient way.", "In this paper, we propose a CRF-CNN framework which can simultaneously model structural information in both output and hidden feature layers in a probabilistic way, and it is applied to human pose estimation.", "Conclusion We propose to use CRF for modeling structured features and structured human body part configurations.", "In contrast, our proposed full CRF-CNN model takes feature-output, output-output, and feature-feature relationships into consideration, which is novel in pose estimation.", "Human pose estimation is to estimate body joint locations from 2D images, which could be applied to assist other tasks such as {{cite:71b7254e-7448-46d2-96a9-124ee9d4efa1}}, {{cite:15566a9d-5693-429c-b58e-864e2ba77bea}}, {{cite:7402a231-5b79-43e5-93ab-a0b9978ac104}} The very first attempt adopting CNN for human pose estimation is DeepPose {{cite:62c83b7c-f2e5-42a5-b3c1-b9788087e79c}}.", "It also facilitates us in borrowing the idea behind the sum-product algorithm and developing a message passing scheme so that each body joint receives messages from all the others in an efficient way by saving intermediate messages."], "common_annotated_sentences_index": [33, 34, 41, 10, 11, 205, 207, 20]}, "2006.14799+9": {"tsa": {"citing_arxiv_id": "2006.14799", "citing_context_list_idx": 9, "citing_context": "These models' weakness of capturing long-span dependencies in long word sequences motivates the development of attention networks MAINCIT and pointer networks CIT . The Transformer architecture CIT , which incorporates an encoder and a decoder, both implemented using the self-attention mechanism, is being adopted by new state-of-the-art NLG systems.", "cited_arxiv_id": "1409.0473", "cited_sentence_idx_list": [14, 15, 17, 23, 70, 71, 72, 73, 148, 149, 213], "cited_sentences": ["A potential issue with this encoder\u2013decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector.", "This may make it difficult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus.", "In order to address this issue, we introduce an extension to the encoder\u2013decoder model which learns to align and translate jointly.", "We show this allows a model to cope better with long sentences.", "Intuitively, this implements a mechanism of attention in the decoder.", "The decoder decides parts of the source sentence to pay attention to.", "By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixed-length vector.", "With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.", "REF  the proposed model (RNNsearch) is much better than the conventional model (RNNencdec) at translating long sentences.", "This is likely due to the fact that the RNNsearch does not require encoding a long sentence into a fixed-length vector perfectly, but only accurately encoding the parts of the input sentence that surround a particular word.", "This gated unit is similar to a long short-term memory (LSTM) unit proposed earlier by {{cite:4f0585d0-26c7-4c86-bedc-f92691fd8f5a}}, sharing with it the ability to better model and learn long-term dependencies."]}, "syu": {"citing_arxiv_id": "2006.14799", "citing_context_list_idx": 9, "citing_context": "These models' weakness of capturing long-span dependencies in long word sequences motivates the development of attention networks MAINCIT and pointer networks CIT . The Transformer architecture CIT , which incorporates an encoder and a decoder, both implemented using the self-attention mechanism, is being adopted by new state-of-the-art NLG systems.", "cited_arxiv_id": "1409.0473", "cited_sentence_idx_list": [4, 5], "cited_sentences": ["The models proposed recently for neural machine translation often belong to a family of encoder\u2013decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation.", "In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder\u2013decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly."]}, "common_annotated_sentences": ["By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixed-length vector.", "We show this allows a model to cope better with long sentences.", "In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder\u2013decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.", "With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.", "This may make it difficult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus.", "REF  the proposed model (RNNsearch) is much better than the conventional model (RNNencdec) at translating long sentences.", "A potential issue with this encoder\u2013decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector.", "This is likely due to the fact that the RNNsearch does not require encoding a long sentence into a fixed-length vector perfectly, but only accurately encoding the parts of the input sentence that surround a particular word.", "Intuitively, this implements a mechanism of attention in the decoder.", "This gated unit is similar to a long short-term memory (LSTM) unit proposed earlier by {{cite:4f0585d0-26c7-4c86-bedc-f92691fd8f5a}}, sharing with it the ability to better model and learn long-term dependencies.", "The models proposed recently for neural machine translation often belong to a family of encoder\u2013decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation.", "The decoder decides parts of the source sentence to pay attention to.", "In order to address this issue, we introduce an extension to the encoder\u2013decoder model which learns to align and translate jointly."], "common_annotated_sentences_index": [4, 5, 70, 71, 72, 73, 14, 15, 17, 148, 149, 213, 23]}, "1903.03148+10": {"tsa": {"citing_arxiv_id": "1903.03148", "citing_context_list_idx": 10, "citing_context": "However, these methods are less applicable in medical image segmentation with many anatomical labels, as an image signal can pass through the rich networks at low cost, leading to a perfect cycle loss, circumventing the required constraints MAINCIT . Variational Bayes auto-encoders have been used for various tasks to learn probabilistic generative models, and often use convolutional networks CIT .", "cited_arxiv_id": "1703.10593", "cited_sentence_idx_list": [3, 4, 5, 36, 89], "cited_sentences": ["We present an approach for learning to translate an image from a source domain FORMULA  to a target domain FORMULA  in the absence of paired examples.", "Our goal is to learn a mapping FORMULA  such that the distribution of images from FORMULA  is indistinguishable from the distribution FORMULA  using an adversarial loss.", "Because this mapping is highly under-constrained, we couple it with an inverse mapping FORMULA  and introduce a cycle consistency loss to enforce FORMULA  (and vice versa).", "We apply this structural assumption by training both the mapping FORMULA  and FORMULA  simultaneously, and adding a cycle consistency loss\u00a0{{cite:c22912de-b2dd-47ff-b067-99e4cc7a2bc0}} that encourages FORMULA  and FORMULA .", "Cycle Consistency Loss Adversarial training can, in theory, learn mappings FORMULA  and FORMULA  that produce outputs identically distributed as target domains FORMULA  and FORMULA  respectively (strictly speaking, this requires FORMULA  and FORMULA  to be stochastic functions)\u00a0{{cite:6292e77e-f107-4051-8d3d-2d47f4e56a3e}}."]}, "syu": {"citing_arxiv_id": "1903.03148", "citing_context_list_idx": 10, "citing_context": "However, these methods are less applicable in medical image segmentation with many anatomical labels, as an image signal can pass through the rich networks at low cost, leading to a perfect cycle loss, circumventing the required constraints MAINCIT . Variational Bayes auto-encoders have been used for various tasks to learn probabilistic generative models, and often use convolutional networks CIT .", "cited_arxiv_id": "1703.10593", "cited_sentence_idx_list": [0, 5, 36, 180], "cited_sentences": ["[  Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Jun-Yan Zhuempty Taesung Park[1] Phillip Isola Alexei A. Efros Berkeley AI Research (BAIR) laboratory, UC Berkeley 2020/09/12 02:14:04   FIGURE  figure Given any two unordered image collections FORMULA  and FORMULA , our algorithm learns to automatically \u201ctranslate\u201d an image from one into the other and vice versa: (left) Monet paintings and landscape photos from Flickr; (center) zebras and horses from ImageNet; (right) summer and winter Yosemite photos from Flickr.", "Because this mapping is highly under-constrained, we couple it with an inverse mapping FORMULA  and introduce a cycle consistency loss to enforce FORMULA  (and vice versa).", "We apply this structural assumption by training both the mapping FORMULA  and FORMULA  simultaneously, and adding a cycle consistency loss\u00a0{{cite:c22912de-b2dd-47ff-b067-99e4cc7a2bc0}} that encourages FORMULA  and FORMULA .", "We also evaluate our method with the cycle loss in only one direction: GAN + forward cycle loss FORMULA , or GAN + backward cycle loss FORMULA  (Equation\u00a0REF ) and find that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed."]}, "common_annotated_sentences": ["Because this mapping is highly under-constrained, we couple it with an inverse mapping FORMULA  and introduce a cycle consistency loss to enforce FORMULA  (and vice versa).", "We apply this structural assumption by training both the mapping FORMULA  and FORMULA  simultaneously, and adding a cycle consistency loss\u00a0{{cite:c22912de-b2dd-47ff-b067-99e4cc7a2bc0}} that encourages FORMULA  and FORMULA .", "We present an approach for learning to translate an image from a source domain FORMULA  to a target domain FORMULA  in the absence of paired examples.", "Our goal is to learn a mapping FORMULA  such that the distribution of images from FORMULA  is indistinguishable from the distribution FORMULA  using an adversarial loss.", "Cycle Consistency Loss Adversarial training can, in theory, learn mappings FORMULA  and FORMULA  that produce outputs identically distributed as target domains FORMULA  and FORMULA  respectively (strictly speaking, this requires FORMULA  and FORMULA  to be stochastic functions)\u00a0{{cite:6292e77e-f107-4051-8d3d-2d47f4e56a3e}}.", "[  Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Jun-Yan Zhuempty Taesung Park[1] Phillip Isola Alexei A. Efros Berkeley AI Research (BAIR) laboratory, UC Berkeley 2020/09/12 02:14:04   FIGURE  figure Given any two unordered image collections FORMULA  and FORMULA , our algorithm learns to automatically \u201ctranslate\u201d an image from one into the other and vice versa: (left) Monet paintings and landscape photos from Flickr; (center) zebras and horses from ImageNet; (right) summer and winter Yosemite photos from Flickr.", "We also evaluate our method with the cycle loss in only one direction: GAN + forward cycle loss FORMULA , or GAN + backward cycle loss FORMULA  (Equation\u00a0REF ) and find that it often incurs training instability and causes mode collapse, especially for the direction of the mapping that was removed."], "common_annotated_sentences_index": [0, 3, 4, 5, 36, 180, 89]}, "1809.04570+250": {"tsa": {"citing_arxiv_id": "1809.04570", "citing_context_list_idx": 250, "citing_context": "It has been demonstrated that moving from floating-point arithmetic to low-precision integer arithmetic only impacts the network accuracy lightly, especially if the network is retrained MAINCIT . The quantization of neural networks down to 8-bit integers has been widely adopted and is well supported by designated and optimized software libraries, such as gemmlowp CIT and the ARM Compute Library CIT .", "cited_arxiv_id": "1511.06488", "cited_sentence_idx_list": [14, 15, 126, 127, 128, 178, 183, 187, 188, 189], "cited_sentences": ["Early studies on word-length determination of neural networks reported the needed precision of at least 8 bits {{cite:c9fc6901-b760-4416-9b69-77d86aad96a3}}.", "Our recent works show that the precision required for implementing FFDNN, CNN or RNN needs not be very high, especially when the quantized networks are trained again to learn the effects of lowered precision.", "REF  shows the performance of direct quantization with 2, 4, 6, and 8-bit precision when the network complexity varies.", "In the FFDNN, 6 bit direct quantization seems enough when the network size is larger than 128.", "But, small FFDNNs demand 8 bits for near floating-point performance.", "The remaining question is how much memory space can be saved by quantization while maintaining the accuracy.", "Specifically, by varying the size, we find the number of total parameters of the floating-point network that shows the same accuracy as the quantized one.", "For the direct quantization, 5 bit quantization shows the best ECR except for the layer size of 1024.", "On the other hand, even 2 bit quantization performs better than the others after retraining.", "That is, after retraining, a bigger network with extreme ternary (2 bit) quantization is more efficient in terms of the memory usage for weights than any other smaller networks with higher quantization bits when they are compared at the same accuracy."]}, "syu": {"citing_arxiv_id": "1809.04570", "citing_context_list_idx": 250, "citing_context": "It has been demonstrated that moving from floating-point arithmetic to low-precision integer arithmetic only impacts the network accuracy lightly, especially if the network is retrained MAINCIT . The quantization of neural networks down to 8-bit integers has been widely adopted and is well supported by designated and optimized software libraries, such as gemmlowp CIT and the ARM Compute Library CIT .", "cited_arxiv_id": "1511.06488", "cited_sentence_idx_list": [6, 19], "cited_sentences": ["We find that some performance gap exists between the floating-point and the retrain-based ternary (+1, 0, -1) weight neural networks when the size is not large enough, but the discrepancy almost vanishes in fully complex networks whose capability is limited by the training data, rather than by the number of connections.", "For this study, the network complexity is changed to analyze their effects on the performance gap between floating-point and retrained low-precision fixed-point deep neural networks."]}, "common_annotated_sentences": ["Early studies on word-length determination of neural networks reported the needed precision of at least 8 bits {{cite:c9fc6901-b760-4416-9b69-77d86aad96a3}}.", "For the direct quantization, 5 bit quantization shows the best ECR except for the layer size of 1024.", "The remaining question is how much memory space can be saved by quantization while maintaining the accuracy.", "Our recent works show that the precision required for implementing FFDNN, CNN or RNN needs not be very high, especially when the quantized networks are trained again to learn the effects of lowered precision.", "On the other hand, even 2 bit quantization performs better than the others after retraining.", "We find that some performance gap exists between the floating-point and the retrain-based ternary (+1, 0, -1) weight neural networks when the size is not large enough, but the discrepancy almost vanishes in fully complex networks whose capability is limited by the training data, rather than by the number of connections.", "REF  shows the performance of direct quantization with 2, 4, 6, and 8-bit precision when the network complexity varies.", "For this study, the network complexity is changed to analyze their effects on the performance gap between floating-point and retrained low-precision fixed-point deep neural networks.", "Specifically, by varying the size, we find the number of total parameters of the floating-point network that shows the same accuracy as the quantized one.", "But, small FFDNNs demand 8 bits for near floating-point performance.", "That is, after retraining, a bigger network with extreme ternary (2 bit) quantization is more efficient in terms of the memory usage for weights than any other smaller networks with higher quantization bits when they are compared at the same accuracy.", "In the FFDNN, 6 bit direct quantization seems enough when the network size is larger than 128."], "common_annotated_sentences_index": [128, 6, 14, 15, 178, 19, 183, 187, 188, 189, 126, 127]}, "1805.06880+251": {"tsa": {"citing_arxiv_id": "1805.06880", "citing_context_list_idx": 251, "citing_context": "Human3.6M As noted in MAINCIT , many different evaluation protocols have been defined for Human3.6M, making it very challenging to comprehensively compare to all existing methods. We opt to use protocol #2 from the state-of-the-art CIT , as it is the model most similar to ours, and has been used by several recent approaches.", "cited_arxiv_id": "1612.06524", "cited_sentence_idx_list": [102, 103, 104], "cited_sentences": ["Evaluation protocols We use Human3.6M for quantitative evaluation and analysis.", "It appears that multiple train/test splits have been used in the literature, as well as different approaches to computing mean per joint position error (MPJPE), measured in millimeters.", "We summarize them here."]}, "syu": {"citing_arxiv_id": "1805.06880", "citing_context_list_idx": 251, "citing_context": "Human3.6M As noted in MAINCIT , many different evaluation protocols have been defined for Human3.6M, making it very challenging to comprehensively compare to all existing methods. We opt to use protocol #2 from the state-of-the-art CIT , as it is the model most similar to ours, and has been used by several recent approaches.", "cited_arxiv_id": "1612.06524", "cited_sentence_idx_list": [29, 102, 103], "cited_sentences": ["Evaluation: Though we present qualitative results on in-the-wild-imagery, we also perform an extensive quantitative evaluation of our method on widely benchmarked 3D human-pose datasets, such as Human3.6M {{cite:cdf7cb8b-1195-418e-84fe-4ff690b35bef}}.", "Evaluation protocols We use Human3.6M for quantitative evaluation and analysis.", "It appears that multiple train/test splits have been used in the literature, as well as different approaches to computing mean per joint position error (MPJPE), measured in millimeters."]}, "common_annotated_sentences": ["Evaluation: Though we present qualitative results on in-the-wild-imagery, we also perform an extensive quantitative evaluation of our method on widely benchmarked 3D human-pose datasets, such as Human3.6M {{cite:cdf7cb8b-1195-418e-84fe-4ff690b35bef}}.", "We summarize them here.", "Evaluation protocols We use Human3.6M for quantitative evaluation and analysis.", "It appears that multiple train/test splits have been used in the literature, as well as different approaches to computing mean per joint position error (MPJPE), measured in millimeters."], "common_annotated_sentences_index": [104, 29, 102, 103]}, "1901.01421+306": {"tsa": {"citing_arxiv_id": "1901.01421", "citing_context_list_idx": 306, "citing_context": "However, in mmWave massive MIMO systems, fully digital precoding is impractical since the number of antennas is large and the working frequency is much higher than that of conventional MIMO systems MAINCIT . In this context, hybrid precoding, including analog and digital precoding, is usually adopted for mmWave massive MIMO communications CIT , CIT , CIT , CIT .", "cited_arxiv_id": "1305.2460", "cited_sentence_idx_list": [24, 29, 30, 31], "cited_sentences": ["A main differentiating factor in mmWave communication is that the ten-fold increase in carrier frequency, compared to the current majority of wireless systems, implies that mmWave signals experience an orders-of-magnitude increase in free-space pathloss.", "For example, traditional multiple-input multiple-output (MIMO) processing is often performed digitally at baseband, which enables controlling both the signal's phase and amplitude.", "Digital processing, however, requires dedicated baseband and RF hardware for each antenna element.", "Unfortunately, the high cost and power consumption of mmWave mixed-signal hardware precludes such a transceiver architecture at present, and forces mmWave systems to rely heavily on analog or RF processing {{cite:14d1527d-f262-422d-b43a-013e90000afc}}, {{cite:2d363b0c-762b-4899-80ca-99ba6f6e081f}}."]}, "syu": {"citing_arxiv_id": "1901.01421", "citing_context_list_idx": 306, "citing_context": "However, in mmWave massive MIMO systems, fully digital precoding is impractical since the number of antennas is large and the working frequency is much higher than that of conventional MIMO systems MAINCIT . In this context, hybrid precoding, including analog and digital precoding, is usually adopted for mmWave massive MIMO communications CIT , CIT , CIT , CIT .", "cited_arxiv_id": "1305.2460", "cited_sentence_idx_list": [326], "cited_sentences": ["When considering the fact that practical mmWave systems will use twenty to fifty times more antennas compared to traditional MIMO systems, which use about 4 to 6 bits of feedback\u00a0{{cite:2b75f029-0594-4341-815b-bd101721eace}}, we see that exploiting spatial sparsity in precoding helps dramatically compress feedback and keep its overhead manageable."]}, "common_annotated_sentences": ["When considering the fact that practical mmWave systems will use twenty to fifty times more antennas compared to traditional MIMO systems, which use about 4 to 6 bits of feedback\u00a0{{cite:2b75f029-0594-4341-815b-bd101721eace}}, we see that exploiting spatial sparsity in precoding helps dramatically compress feedback and keep its overhead manageable.", "Unfortunately, the high cost and power consumption of mmWave mixed-signal hardware precludes such a transceiver architecture at present, and forces mmWave systems to rely heavily on analog or RF processing {{cite:14d1527d-f262-422d-b43a-013e90000afc}}, {{cite:2d363b0c-762b-4899-80ca-99ba6f6e081f}}.", "A main differentiating factor in mmWave communication is that the ten-fold increase in carrier frequency, compared to the current majority of wireless systems, implies that mmWave signals experience an orders-of-magnitude increase in free-space pathloss.", "Digital processing, however, requires dedicated baseband and RF hardware for each antenna element.", "For example, traditional multiple-input multiple-output (MIMO) processing is often performed digitally at baseband, which enables controlling both the signal's phase and amplitude."], "common_annotated_sentences_index": [326, 24, 29, 30, 31]}, "1908.11030+331": {"tsa": {"citing_arxiv_id": "1908.11030", "citing_context_list_idx": 331, "citing_context": "While BERT supports sequence lengths up to 512, a shorter sequence length is recommended by Google Research CIT as the relationship between Transformer attention and sequence length is quadratic MAINCIT , leading to dramatic increases in computation time. A training batch size of 64 is used to maximize the efficiency of the TPU v3-8.", "cited_arxiv_id": "1810.04805", "cited_sentence_idx_list": [2, 267], "cited_sentences": ["Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout}@google.com We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.", "Longer sequences are disproportionately expensive because attention is quadratic to the sequence length."]}, "syu": {"citing_arxiv_id": "1908.11030", "citing_context_list_idx": 331, "citing_context": "While BERT supports sequence lengths up to 512, a shorter sequence length is recommended by Google Research CIT as the relationship between Transformer attention and sequence length is quadratic MAINCIT , leading to dramatic increases in computation time. A training batch size of 64 is used to maximize the efficiency of the TPU v3-8.", "cited_arxiv_id": "1810.04805", "cited_sentence_idx_list": [267], "cited_sentences": ["Longer sequences are disproportionately expensive because attention is quadratic to the sequence length."]}, "common_annotated_sentences": ["Longer sequences are disproportionately expensive because attention is quadratic to the sequence length.", "Kristina Toutanova Google AI Language {jacobdevlin,mingweichang,kentonl,kristout}@google.com We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers."], "common_annotated_sentences_index": [2, 267]}}